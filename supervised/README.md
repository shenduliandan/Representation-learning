# Research
1. Learning Transferable Visual Models From Natural Language Supervisionï¼š[Paper](https://arxiv.org/abs/2103.00020) [Code](https://github.com/openai/CLIP)
2. Open-vocabulary Object Detection via Vision and Language Knowledge Distillation: [Paper](https://arxiv.org/abs/2104.13921) [Code](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)
3. CLIP-Adapter: Better Vision-Language Models with Feature Adapters: [Paper](https://arxiv.org/abs/2110.04544) [Code](https://github.com/gaopengcuhk/clip-adapter)
4. CLIP4Caption: CLIP for Video Caption: [Paper](https://arxiv.org/abs/2110.06615) [Code]()
5. A Simple Baseline for Zero-shot Semantic Segmentation with Pre-trained Vision-language Model: [Paper](https://arxiv.org/abs/2112.14757) [Code](https://github.com/MendelXu/zsseg.baseline)
