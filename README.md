# Research domain
## Retrieval
1. Learning Transferable Visual Models From Natural Language Supervision [Paper](https://arxiv.org/abs/2103.00020) [Code](https://github.com/openai/CLIP)
2. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision [Paper](https://arxiv.org/abs/2102.05918)
3. Data-Efficient Language-Supervised Zero-Shot Learning with Self-Distillation [Paper](https://arxiv.org/abs/2104.08945)
4. Better Vision-Language Models with Feature Adapters [Paper](https://arxiv.org/abs/2110.04544) [Code](https://github.com/gaopengcuhk/clip-adapter)

## Detection&Segmentation
1. A Simple Baseline for Zero-shot Semantic Segmentation with Pre-trained Vision-language Model [Paper](https://arxiv.org/abs/2112.14757) [Code](https://github.com/MendelXu/zsseg.baseline)
2. Open-vocabulary Object Detection via Vision and Language Knowledge Distillation [Paper](https://arxiv.org/abs/2104.13921) [Code](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)

## Caption
1. CLIP for Video Caption [Paper](https://arxiv.org/abs/2110.06615)
